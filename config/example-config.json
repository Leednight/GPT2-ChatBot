{
  "architectures": [
    "BertForMaskedLM"                           # 模型的名称
  ],
  "attention_probs_dropout_prob": 0.1,          # 注意力机制的 dropout，默认为0.1
  "directionality": "bidi",                     # 文字编码方向采用bidi算法
  "hidden_act": "gelu",                         # 编码器内激活函数，默认"gelu"，还可为"relu"、"swish"或 "gelu_new"
  "hidden_dropout_prob": 0.1,                   # 词嵌入层或编码器的 dropout，默认为0.1
  "hidden_size": 768,                           # 编码器内隐藏层神经元数量，默认768
  "initializer_range": 0.02,                    # 神经元权重的标准差，默认为0.02
  "intermediate_size": 3072,                    # 编码器内全连接层的输入维度，默认3072
  "layer_norm_eps": 1e-12,                      # layer normalization 的 epsilon 值，默认为 1e-12
  "max_position_embeddings": 512,               # 模型使用的最大序列长度，默认为512
  "model_type": "bert",                         # 模型类型是bert
  "num_attention_heads": 12,                    # 编码器内注意力头数，默认12
  "num_hidden_layers": 12,                      # 编码器内隐藏层层数，默认12
  "pad_token_id": 0,                            # pad_token_id 未找到相关解释
  "pooler_fc_size": 768,                        # 下面应该是pooler层的参数，本质是个全连接层，作为分类器解决序列级的NLP任务
  "pooler_num_attention_heads": 12,             # pooler层注意力头，默认12
  "pooler_num_fc_layers": 3,                    # pooler 连接层数，默认3
  "pooler_size_per_head": 128,                  # 每个注意力头的size
  "pooler_type": "first_token_transform",       # pooler层类型，网上介绍很少
  "type_vocab_size": 2,                         # 词汇表类别，默认为2
  "vocab_size": 21128                           # 词汇数，bert默认30522，这是因为bert以中文字为单位进入输入
}